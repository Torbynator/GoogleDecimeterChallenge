{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a vewrsion of the original notebook. Instead of preparing the data by padding and masking smaller batches of time series of the trajectories are prepared and fed to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for the GoogleDecimeterChallenge https://www.kaggle.com/competitions/smartphone-decimeter-2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try to run it on google collab, works great only RAM is limited:\n",
    "https://colab.research.google.com/github/Torbynator/GoogleDecimeterChallenge/blob/main/main.ipynb#scrollTo=TOn-Can4C0YP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Only for google collab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install  kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "!kaggle competitions download -c smartphone-decimeter-2023\n",
    "!unzip /content/smartphone-decimeter-2023.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\t\n",
    "import folium\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "INPUT_PATH = 'sdc2023/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_input_data = []\n",
    "# test_gt_data = []\n",
    "\n",
    "# #iterate over all data files and store them in the respective arrays\n",
    "\n",
    "# #load test data\n",
    "# test_files = os.listdir(INPUT_PATH + \"test\")\n",
    "\n",
    "# for folder in test_files:\n",
    "#     smartphones = os.listdir(INPUT_PATH + \"test/\"+folder)\n",
    "#     for smartphone in smartphones:\n",
    "#         file =  \"/device_gnss.csv\"\n",
    "#         #store data in list while dropping first and 41st column (string data)\n",
    "#         test_input_data.append(pd.read_csv(INPUT_PATH + \"test/\" +folder+\"/\"+smartphone + file, usecols=[i for i in range(58) if i not in [0,40]], dtype=np.float32).to_numpy(dtype=np.float32).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "\n",
    "def read_data(MAX_TRAJECTORIES):\n",
    "    train_files = os.listdir(INPUT_PATH + \"train\")\n",
    "    trajectory_count=0\n",
    "    used_columns = [\"utcTimeMillis\",\"RawPseudorangeMeters\", \"RawPseudorangeUncertaintyMeters\" ,\"SvPositionXEcefMeters\" ,\"SvPositionYEcefMeters\", \"SvPositionZEcefMeters\", \"IsrbMeters\"]\n",
    "    \n",
    "    train_input_data = []\n",
    "    train_gt_data = []\n",
    "\n",
    "    for folder in train_files:\n",
    "        smartphones = os.listdir(INPUT_PATH + \"train/\"+folder)\n",
    "        for smartphone in smartphones:\n",
    "            files = os.listdir(INPUT_PATH + \"train/\"+folder+\"/\"+smartphone)\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    if trajectory_count >= MAX_TRAJECTORIES:\n",
    "                        break   \n",
    "                    if \"gnss\" in file:\n",
    "                        #store data in list while dropping first and 41st column (string data), as well as (porbably mostly) empty columns\n",
    "                        #also all data points with the same timestep are seen as features of one timestep of a sample\n",
    "                        sample = pd.read_csv(INPUT_PATH + \"train/\" +folder+\"/\"+ smartphone+ \"/\" + file, usecols=used_columns, dtype=float).to_numpy(dtype=float)\n",
    "                        #correct PseudoRange with ISRB\n",
    "                        sample[:,1] = sample[:,1] + sample[:,6]\n",
    "                        train_input_data.append(sample[:,0:6].swapaxes(0,1))                    \n",
    "                    elif \"ground_truth\" in file:\n",
    "                        trajectory_count +=1\n",
    "                        #store data in list while dropping first and 2nd column (string data),(porbably mostly) empty columns\n",
    "                        train_gt_data.append(pd.read_csv(INPUT_PATH + \"train/\"+folder+\"/\" + smartphone+ \"/\" + file,  usecols=[i for i in range(9) if i not in [0,1]], dtype=float).to_numpy(dtype=float).swapaxes(0,1).tolist())\n",
    "                        print(f\"read in {trajectory_count} samples\")\n",
    "\n",
    "    #delete IRSB column\n",
    "    #del train_input_data[:][6]\n",
    "\n",
    "    #replace NaN values with 0\n",
    "    train_input_data = [[[0 if math.isnan(x) else x for x in timestep] for timestep in sample ] for sample in train_input_data]\n",
    "\n",
    "    return train_input_data, train_gt_data\n",
    "\n",
    "def read_imu_data(gnss_data_sorted_not_batched, MAX_TRAJECTORIES):\n",
    "    train_files = os.listdir(INPUT_PATH + \"train\")\n",
    "    trajectory_count=0\n",
    "    used_columns_IMU = [\"MessageType\",\"utcTimeMillis\",\"MeasurementX\",\"MeasurementY\",\"MeasurementZ\"]\n",
    "    imu_data = []\n",
    "    \n",
    "    for folder in train_files:\n",
    "        smartphones = os.listdir(INPUT_PATH + \"train/\"+folder)\n",
    "        for smartphone in smartphones:\n",
    "            files = os.listdir(INPUT_PATH + \"train/\"+folder+\"/\"+smartphone)\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    if trajectory_count >= MAX_TRAJECTORIES:\n",
    "                        break   \n",
    "                    if \"imu\" in file:\n",
    "                        #calculate accelerometer and gyro average measurement, sum and variance between gnss timesteps and store them in array with timesteps of gnss measurements\n",
    "                        #timestep of the calculated values is the later timestep of the used gnss timesteps\n",
    "                        #calculated values will later be added to the train data as features at the corresponding timesteps\n",
    "                        sample = pd.read_csv(INPUT_PATH + \"train/\" +folder+\"/\"+ smartphone+ \"/\" + file, usecols=used_columns_IMU).to_numpy()\n",
    "                        gnss_samples = gnss_data_sorted_not_batched[trajectory_count]\n",
    "                        sample_calc = []\n",
    "                        \n",
    "                        sample_iterator = 0\n",
    "                        measurement = sample[0]\n",
    "                        for gnss_step in gnss_samples:                            \n",
    "                            acc_values = [[],[],[]]\n",
    "                            gyro_values = [[],[],[]]\n",
    "                            while measurement[1] < gnss_step[0]:\n",
    "                                if \"Acc\" in measurement[0]:\n",
    "                                    acc_values[0].append(measurement[2])\n",
    "                                    acc_values[1].append(measurement[3])\n",
    "                                    acc_values[2].append(measurement[4])\n",
    "                                elif \"Gyr\" in measurement[0]:\n",
    "                                    gyro_values[0].append(measurement[2])\n",
    "                                    gyro_values[1].append(measurement[3])\n",
    "                                    gyro_values[2].append(measurement[4])\n",
    "                                sample_iterator += 1 \n",
    "                                measurement = sample[sample_iterator]    \n",
    "                                \n",
    "                            if any(acc_values) and any(gyro_values):  \n",
    "                                acc_average = [np.mean(acc_values[0]), np.mean(acc_values[1]), np.mean(acc_values[2])]\n",
    "                                acc_variance = [np.var(acc_values[0]), np.var(acc_values[1]), np.var(acc_values[2])]\n",
    "                                #acc_sum = [np.sum(acc_values[0]), np.sum(acc_values[1]), np.sum(acc_values[2])]\n",
    "                                gyro_average = [np.mean(gyro_values[0]), np.mean(gyro_values[1]), np.mean(gyro_values[2])]\n",
    "                                gyro_variance = [np.var(gyro_values[0]), np.var(gyro_values[1]), np.var(gyro_values[2])]\n",
    "                                #gyro_sum = [np.sum(gyro_values[0]), np.sum(gyro_values[1]), np.sum(gyro_values[2])]\n",
    "                                #sample_calc.append([acc_average[0], acc_average[1], acc_average[2], acc_variance[0], acc_variance[1], acc_variance[2], acc_sum[0], acc_sum[1], acc_sum[2], gyro_average[0], gyro_average[1], gyro_average[2], gyro_variance[0], gyro_variance[1], gyro_variance[2], gyro_sum[0], gyro_sum[1], gyro_sum[2]])\n",
    "                                sample_calc.append([acc_average[0], acc_average[1], acc_average[2], acc_variance[0], acc_variance[1], acc_variance[2], gyro_average[0], gyro_average[1], gyro_average[2], gyro_variance[0], gyro_variance[1], gyro_variance[2]])\n",
    "                            else:\n",
    "                                #sample_calc.append([0]*18)\n",
    "                                sample_calc.append([0,9.81,0,0,0,0,0,0,0,0,0,0])\n",
    "                        imu_data.append(np.array(sample_calc).swapaxes(0,1))\n",
    "                        trajectory_count +=1\n",
    "                        print(f\"read in {trajectory_count} samples\")\n",
    "    return imu_data\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(data, identifier):\n",
    "    '''normalize data and return the scaler objects\n",
    "    normalizes data to be roughly between 0 and 1\n",
    "    or sometimes -1 and 1 in manner that makes the scaled data interpretable\n",
    "    \n",
    "    Args:\n",
    "    data to be normalized\n",
    "    type of data, to use standard predifenied normalization methods, since the type of data is knwon\n",
    "\n",
    "    Returns:\n",
    "    normalized data and scaler\n",
    "    '''\n",
    "    normalized_data = []\n",
    "    scaler = -1 #initialize undefined scaler\n",
    "\n",
    "    #differentiate which data comes is\n",
    "    if identifier == \"gnss\":\n",
    "        print(\"normalizing gnss data\")\n",
    "        #utc time gets ignored\n",
    "        #pseudorange is scaled (x-18*10^6m)/10^6m (satellites are at 20*10^6m orbital height. received signals dont go over 30*10^6m)\n",
    "        #pseudorange uncertainty is scaled by 25m (a typically high value)\n",
    "        #satellite postion is scaled by 20^6m which is a orbital heigth\n",
    "        for dataset in data:\n",
    "            normalized_feature = []\n",
    "            for i, feature in enumerate(np.array(dataset)):\n",
    "                if i == 1: #raw pseudorange-lsrb\n",
    "                    feature = (feature-18*10**6)/10**6\n",
    "                if i == 2:\n",
    "                    feature = feature/25\n",
    "                if i == 3 or i == 4 or i == 5: #satellite position\n",
    "                    feature = feature/10**6\n",
    "                normalized_feature.append(feature)\n",
    "            normalized_feature = np.array(normalized_feature).swapaxes(0,1).tolist()\n",
    "            normalized_data.append(normalized_feature)\n",
    "    \n",
    "\n",
    "    elif identifier == \"IMU\":\n",
    "        #acceleration average gets scaled by 9.81m/s^2\n",
    "        #gravity gets taken into account in y-direction\n",
    "        #everything else gets ignored, since the data is already in a sensible range\n",
    "        print(\"normalizing IMU data\")\n",
    "        for dataset in data:\n",
    "            normalized_feature = []\n",
    "            for i, feature in enumerate(np.array(dataset)):\n",
    "                if i == 0 or i ==2:\n",
    "                    feature = feature/9.81\n",
    "                if i == 1:\n",
    "                    feature = feature/9.81-1\n",
    "                normalized_feature.append(feature)\n",
    "            normalized_feature = np.array(normalized_feature).swapaxes(0,1).tolist()\n",
    "            normalized_data.append(normalized_feature)\n",
    "\n",
    "    elif identifier == \"gt\":\n",
    "        print(\"normalizing gt data\")\n",
    "        #latitude and longitude are scaled by 180°\n",
    "        #altitude is scaled by 12800m (flight level 420, the highest commercial flight level)\n",
    "        #speed is scaled by the speed of sound (343m/s)\n",
    "        #accuracy gets ignored\n",
    "        #bearing degrees are scaled by 360°\n",
    "        #utc time gets ignored\n",
    "        for dataset in data:\n",
    "            normalized_feature = []\n",
    "            for i, feature in enumerate(np.array(dataset)):\n",
    "                if i == 0 or i == 1: #latitude and longitude\n",
    "                    feature = feature/180\n",
    "                if i == 2: #altitude\n",
    "                    feature = feature/12800\n",
    "                if i == 3: #speed\n",
    "                    feature = feature/343\n",
    "                if i == 5: #bearing\n",
    "                    feature = feature/360\n",
    "                normalized_feature.append(feature)\n",
    "            normalized_feature = np.array(normalized_feature).swapaxes(0,1).tolist()\n",
    "            normalized_data.append(normalized_feature)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print(\"normalizing arbitrary data\")\n",
    "        scaler = preprocess.MinMaxScaler()\n",
    "        for sample in data:\n",
    "            data_range = []\n",
    "            normalized_data.append(scaler.fit_transform(sample))\n",
    "        normalized_data=np.array(normalized_data)\n",
    "\n",
    "    \n",
    "\n",
    "    return normalized_data    \n",
    "    \n",
    "    \n",
    "    # old function\n",
    "    # #normalize data\n",
    "    # print(\"normalizing data\")\n",
    "    # scaler = preprocess.MinMaxScaler()\n",
    "    # train_input_data_padded_normalized = []\n",
    "    # for sample in train_input_data_padded:\n",
    "    #     data_range = []\n",
    "    #     train_input_data_padded_normalized.append(scaler.fit_transform(sample))\n",
    "    # train_input_data_padded_normalized=np.array(train_input_data_padded_normalized)\n",
    "\n",
    "    # gt_scaler = preprocess.MinMaxScaler()\n",
    "    # train_gt_data_padded_normalized = []\n",
    "    # for sample in train_gt_data_padded:\n",
    "    #     data_range = []\n",
    "    #     train_gt_data_padded_normalized.append(gt_scaler.fit_transform(sample))\n",
    "    # train_gt_data_padded_normalized=np.array(train_gt_data_padded_normalized)\n",
    "\n",
    "    # return train_input_data_padded_normalized, train_gt_data_padded_normalized, scaler, gt_scaler\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME_SERIES_SIZE = 50\n",
    "\n",
    "def sort_data(TIME_SERIES_SIZE, train_input_data, train_gt_data, MAX_TRAJECTORIES):\n",
    "    #move all features with the same timestamp to one line \n",
    "    for sample in range(len(train_input_data)):\n",
    "        matches = 0\n",
    "        print(f\"sample {sample+1} sorted\")\n",
    "        timesteps = 0\n",
    "        for step in range(len(train_input_data[sample])):\n",
    "            if step != 0:\n",
    "                if train_input_data[sample][step-matches][0] == train_input_data[sample][step-1-matches][0]:\n",
    "                    train_input_data[sample][step-1-matches].extend(train_input_data[sample][step-matches][1:])\n",
    "                    train_input_data[sample].pop(step-matches)\n",
    "                    matches +=1\n",
    "                else:\n",
    "                    timesteps+=1\n",
    "\n",
    "\n",
    "    #read in imu data and add it to the train data\n",
    "    print(\"reading imu data\")\n",
    "    IMU_data = read_imu_data(train_input_data, MAX_TRAJECTORIES)\n",
    "    IMU_data = normalize_data(IMU_data, \"IMU\")\n",
    "\n",
    "    \n",
    "\n",
    "    #prepare batches of time series of size TIME_SERIES_SIZE\n",
    "    print(\"batching data\")\n",
    "    train_input_data_batched = [sample[i:i+TIME_SERIES_SIZE] for sample in train_input_data for i in range(0,len(sample)-TIME_SERIES_SIZE)]\n",
    "    train_gt_data_batched = [sample[i:i+TIME_SERIES_SIZE] for sample in train_gt_data for i in range(0,len(sample)-TIME_SERIES_SIZE)]\n",
    "    IMU_data_batched = [sample[i:i+TIME_SERIES_SIZE] for sample in IMU_data for i in range(0,len(sample)-TIME_SERIES_SIZE)]\n",
    "    IMU_data_batched = np.array(IMU_data_batched)\n",
    "\n",
    "\n",
    "    #pad input data\n",
    "    print(\"padding train data\")\n",
    "    max_features = max(max([len(feature) for sample in train_input_data_batched for feature in sample]),max([len(feature) for sample in train_gt_data_batched for feature in sample]))\n",
    "\n",
    "\n",
    "    train_input_data_padded = [tf.keras.preprocessing.sequence.pad_sequences(sample,value=0, padding=\"post\", dtype=np.float64, maxlen=max_features) for sample in train_input_data_batched]\n",
    "    train_input_data_padded = np.array(train_input_data_padded)\n",
    "\n",
    "    del train_input_data, train_input_data_batched\n",
    "    \n",
    "    print(train_input_data_padded.shape)\n",
    "    print(IMU_data_batched.shape)\n",
    "    #insert IMU data at the end of train data\n",
    "    print(\"inserting IMU data\")\n",
    "    train_input_data_padded = np.concatenate((train_input_data_padded, IMU_data_batched), axis=2)\n",
    "    del IMU_data_batched\n",
    "    print(train_input_data_padded.shape)\n",
    "\n",
    "    #pad ground truth data\n",
    "    print(\"padding gt data\")\n",
    "    max_features_gt = max(len(feature) for sample in train_gt_data_batched for feature in sample)\n",
    "    \n",
    "\n",
    "    #convert to right data format\n",
    "    train_gt_data_padded = [tf.keras.preprocessing.sequence.pad_sequences(sample, value=0,padding=\"post\", dtype=np.float64, maxlen=max_features_gt) for sample in train_gt_data_batched]\n",
    "    train_gt_data_padded = np.array(train_gt_data_padded)\n",
    "    del train_gt_data, train_gt_data_batched\n",
    "\n",
    "    print(train_input_data_padded.shape)\n",
    "    print(train_gt_data_padded.shape)\n",
    "\n",
    "    if(train_input_data_padded.shape[0] > train_gt_data_padded.shape[0]):\n",
    "        train_input_data_padded = train_input_data_padded[:train_gt_data_padded.shape[0]]\n",
    "    elif(train_input_data_padded.shape[0] < train_gt_data_padded.shape[0]):\n",
    "        train_gt_data_padded = train_gt_data_padded[:train_input_data_padded.shape[0]]\n",
    "\n",
    "    print(train_input_data_padded.shape)\n",
    "    print(train_gt_data_padded.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    return train_input_data_padded, train_gt_data_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(lat1, lon1, lat2, lon2):  # generally used geo measurement function\n",
    "    R = 6378.137; # Radius of earth in KM\n",
    "    dLat = lat2 * np.pi / 180 - lat1 * np.pi / 180\n",
    "    dLon = lon2 * np.pi / 180 - lon1 * np.pi / 180\n",
    "    a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(lat1 * np.pi / 180) * np.cos(lat2 * np.pi / 180) *  np.sin(dLon/2) * np.sin(dLon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = R * c\n",
    "    return d * 10000; #decimeters\n",
    "\n",
    "\n",
    "class DecimeterError(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(DecimeterError, self).__init__()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        super().on_epoch_begin(epoch, logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "    \n",
    "    def on_training_begin(self, logs=None):\n",
    "        super().on_training_begin(logs)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        super().on_batch_begin(batch, logs)\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        super().on_batch_end(batch, logs)\n",
    "\n",
    "\n",
    "    def on_training_end(self, epoch, logs=None):\n",
    "        super().on_training_end(epoch, logs)\n",
    "        ly_pred = self.model.predict(train_input_data_padded_normalized)\n",
    "        ly_true = train_gt_data_padded_normalized\n",
    "        ly_pred = np.array([gt_scaler.inverse_transform(sample) for sample in ly_pred])\n",
    "        ly_true = np.array([gt_scaler.inverse_transform(sample) for sample in ly_true])\n",
    "        error = measure(ly_pred[:,:,0], ly_pred[:,:,1], ly_true[:,:,0], ly_true[:,:,1])\n",
    "        total_avg_error = np.mean(error)\n",
    "        print(f\"decimeter error: {total_avg_error}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "SAMPLES = 10\n",
    "\n",
    "def objective(trial):\n",
    "    global train_input_data_padded_normalized, train_gt_data_padded_normalized, scaler\n",
    "    TIME_SERIES_SIZE = trial.suggest_int(\"TIME_SERIES_SIZE\", 3, 50)\n",
    "    train_input_data, train_gt_data = read_data(SAMPLES)\n",
    "    train_gt_data_normalized = normalize_data(train_gt_data, \"gt\")\n",
    "    train_input_data_normalized = normalize_data(train_input_data, \"gnss\")\n",
    "    train_input_data_padded_normalized, train_gt_data_padded_normalized = sort_data(TIME_SERIES_SIZE, train_input_data_normalized, train_gt_data_normalized, SAMPLES)\n",
    "    #train_input_data_padded_normalized, train_gt_data_padded_normalized, scaler, gt_scaler = normalize_data(train_input_data_padded, train_gt_data_padded)\n",
    "\n",
    "    print(\"creating model\")\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 7)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"tanh\", \"relu\", \"linear\"])\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0, input_shape=(train_input_data_padded_normalized.shape[1], train_input_data_padded_normalized.shape[2])))\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(trial.suggest_int(f\"n_units_l{i}\", 10, 500), activation=activation, return_sequences=True))\n",
    "    model.add(Dense(7, activation=\"linear\"))\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate), loss=\"MeanSquaredError\")\n",
    "\n",
    "    print(\"training model\")\n",
    "    history = model.fit(train_input_data_padded_normalized, train_gt_data_padded_normalized, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1, callbacks=[PlotLossesKeras(), DecimeterError()], verbose=1)\n",
    "    #return validation score as indicator for the model quality\n",
    "    print(history.history[\"val_loss\"][-1])\n",
    "    return history.history[\"val_loss\"][-1]\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-29 22:36:42,659] A new study created in memory with name: GNSS_LSTM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in 1 samples\n",
      "read in 2 samples\n",
      "read in 3 samples\n",
      "read in 4 samples\n",
      "read in 5 samples\n",
      "read in 6 samples\n",
      "read in 7 samples\n",
      "read in 8 samples\n",
      "read in 9 samples\n",
      "read in 10 samples\n",
      "normalizing gt data\n",
      "normalizing gnss data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-07-29 22:39:28,004] Trial 0 failed with parameters: {'TIME_SERIES_SIZE': 33} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\klink\\AppData\\Local\\Temp\\ipykernel_14088\\2376077045.py\", line 10, in objective\n",
      "    train_input_data_normalized = normalize_data(train_input_data, \"gnss\")\n",
      "  File \"C:\\Users\\klink\\AppData\\Local\\Temp\\ipykernel_14088\\3795760521.py\", line 34, in normalize_data\n",
      "    normalized_feature = np.array(normalized_feature).swapaxes(0,1).tolist()\n",
      "  File \"C:\\Users\\klink\\AppData\\Local\\Temp\\ipykernel_14088\\3795760521.py\", line 34, in normalize_data\n",
      "    normalized_feature = np.array(normalized_feature).swapaxes(0,1).tolist()\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1457, in _pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 701, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1152, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1135, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 312, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"c:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2070, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"c:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2106, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2024-07-29 22:39:28,006] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNSS_LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[44], line 10\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      8\u001b[0m train_input_data, train_gt_data \u001b[38;5;241m=\u001b[39m read_data(SAMPLES)\n\u001b[0;32m      9\u001b[0m train_gt_data_normalized \u001b[38;5;241m=\u001b[39m normalize_data(train_gt_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m train_input_data_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgnss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m train_input_data_padded_normalized, train_gt_data_padded_normalized \u001b[38;5;241m=\u001b[39m sort_data(TIME_SERIES_SIZE, train_input_data_normalized, train_gt_data_normalized, SAMPLES)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#train_input_data_padded_normalized, train_gt_data_padded_normalized, scaler, gt_scaler = normalize_data(train_input_data_padded, train_gt_data_padded)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 34\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(data, identifier)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     33\u001b[0m             normalized_feature\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[1;32m---> 34\u001b[0m         normalized_feature \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(normalized_feature)\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     35\u001b[0m         normalized_data\u001b[38;5;241m.\u001b[39mappend(normalized_feature)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m identifier \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#acceleration average gets scaled by 9.81m/s^2\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m#gravity gets taken into account in y-direction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#everything else gets ignored, since the data is already in a sensible range\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 34\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(data, identifier)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     33\u001b[0m             normalized_feature\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[1;32m---> 34\u001b[0m         normalized_feature \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(normalized_feature)\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     35\u001b[0m         normalized_data\u001b[38;5;241m.\u001b[39mappend(normalized_feature)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m identifier \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#acceleration average gets scaled by 9.81m/s^2\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m#gravity gets taken into account in y-direction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#everything else gets ignored, since the data is already in a sensible range\u001b[39;00m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\google_decimeter_challenge\\GoogleDecimeterChallenge\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"GNSS_LSTM\")\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"optuna_results\"\n",
    "study_name = \"LSTM for GNSS\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_study = optuna.create_study(study_name=study_name, storage=f\"sqlite:///{save_path}/{study_name}.db\")\n",
    "save_study.add_trials(study.trials)\n",
    "study.trials_dataframe().to_csv(f\"{save_path}/{study_name}.csv\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_study = optuna.load_study(study_name=study_name, storage=f\"sqlite:///{save_path}/{study_name}.db\")\n",
    "print(loaded_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with best params\n",
    "SAMPLES = 20\n",
    "params = loaded_study.best_params\n",
    "\n",
    "global train_input_data_padded_normalized, train_gt_data_padded_normalized, gt_scaler\n",
    "TIME_SERIES_SIZE = params[\"TIME_SERIES_SIZE\"]\n",
    "train_input_data, train_gt_data = read_data(SAMPLES)\n",
    "train_input_data_padded, train_gt_data_padded = sort_data(TIME_SERIES_SIZE, train_input_data, train_gt_data, MAX_TRAJECTORIES=SAMPLES)\n",
    "train_input_data_padded_normalized, train_gt_data_padded_normalized, scaler, gt_scaler = normalize_data(train_input_data_padded, train_gt_data_padded)\n",
    "\n",
    "n_layers = params[\"n_layers\"]\n",
    "activation = params[\"activation\"]\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(train_input_data_padded_normalized.shape[1], train_input_data_padded_normalized.shape[2])))\n",
    "for i in range(n_layers):\n",
    "    model.add(LSTM(params[f\"n_units_l{i}\"] , activation=activation, return_sequences=True))\n",
    "model.add(Dense(7, activation=\"linear\"))\n",
    "\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate), loss=\"MeanSquaredError\")\n",
    "\n",
    "history = model.fit(train_input_data_padded_normalized, train_gt_data_padded_normalized, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[PlotLossesKeras(), DecimeterError()], validation_split=0.2, verbose=1)\n",
    "#return validation score as indicator for the model quality\n",
    "print(history.history[\"val_loss\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data_for_usage(TIME_SERIES_SIZE, train_input_data, train_gt_data, number_of_features,MAX_TRAJECTORIES):\n",
    "    #move all features with the same timestamp to one line \n",
    "    for sample in range(len(train_input_data)):\n",
    "        matches = 0\n",
    "        print(f\"sample {sample+1} sorted\")\n",
    "        timesteps = 0\n",
    "        for step in range(len(train_input_data[sample])):\n",
    "            if step != 0:\n",
    "                if train_input_data[sample][step-matches][0] == train_input_data[sample][step-1-matches][0]:\n",
    "                    train_input_data[sample][step-1-matches].extend(train_input_data[sample][step-matches][1:])\n",
    "                    train_input_data[sample].pop(step-matches)\n",
    "                    matches +=1\n",
    "                else:\n",
    "                    timesteps+=1\n",
    "\n",
    "\n",
    "    #read in imu data and add it to the train data\n",
    "    print(\"reading imu data\")\n",
    "    IMU_data = read_imu_data(train_input_data, MAX_TRAJECTORIES)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #prepare batches of time series of size TIME_SERIES_SIZE\n",
    "    print(\"batching data\")\n",
    "    train_input_data_batched = [sample[i:i+TIME_SERIES_SIZE] for sample in train_input_data for i in range(0,len(sample)-TIME_SERIES_SIZE)]\n",
    "    train_gt_data_batched  = []\n",
    "    for sample in train_gt_data:\n",
    "        train_gt_data_batched.extend([sample[i:i+TIME_SERIES_SIZE] for i in range(0,len(sample)-TIME_SERIES_SIZE)])\n",
    "    train_gt_data_batched = np.array(train_gt_data_batched)\n",
    "    #train_input_data_batched = np.array(train_input_data_batched)\n",
    "    IMU_data_batched = [sample[i:i+TIME_SERIES_SIZE] for sample in IMU_data for i in range(0,len(sample)-TIME_SERIES_SIZE)]\n",
    "    IMU_data_batched = np.array(IMU_data_batched)\n",
    "\n",
    "\n",
    "    #pad input data\n",
    "    print(\"padding train data\")\n",
    "\n",
    "\n",
    "    train_input_data_padded = [tf.keras.preprocessing.sequence.pad_sequences(sample,value=0, padding=\"post\", dtype=np.float64, maxlen=number_of_features) for sample in train_input_data_batched]\n",
    "    train_input_data_padded = np.array(train_input_data_padded)\n",
    "\n",
    "    del train_input_data, train_input_data_batched\n",
    "    \n",
    "    print(train_input_data_padded.shape)\n",
    "    print(IMU_data_batched.shape)\n",
    "    #insert IMU data at the end of train data\n",
    "    print(\"inserting IMU data\")\n",
    "    train_input_data_padded = np.concatenate((train_input_data_padded, IMU_data_batched), axis=2)\n",
    "    del IMU_data_batched\n",
    "    print(train_input_data_padded.shape)\n",
    "\n",
    "    #pad ground truth data\n",
    "    print(\"padding gt data\")\n",
    "    #max_features_gt = max(len(feature) for sample in train_gt_data_batched for feature in sample)\n",
    "    train_gt_data_padded = np.array(train_gt_data_batched)\n",
    "\n",
    "    #convert to right data format\n",
    "    # train_gt_data_padded = [tf.keras.preprocessing.sequence.pad_sequences(sample, value=0,padding=\"post\", dtype=np.float64, maxlen=max_features_gt) for sample in train_gt_data_batched]\n",
    "    # train_gt_data_padded = np.array(train_gt_data_padded)\n",
    "    del train_gt_data, train_gt_data_batched\n",
    "\n",
    "    print(train_input_data_padded.shape)\n",
    "    print(train_gt_data_padded.shape)\n",
    "\n",
    "    if(train_input_data_padded.shape[0] > train_gt_data_padded.shape[0]):\n",
    "        train_input_data_padded = train_input_data_padded[:train_gt_data_padded.shape[0]]\n",
    "    elif(train_input_data_padded.shape[0] < train_gt_data_padded.shape[0]):\n",
    "        train_gt_data_padded = train_gt_data_padded[:train_input_data_padded.shape[0]]\n",
    "\n",
    "    print(train_input_data_padded.shape)\n",
    "    print(train_gt_data_padded.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    return train_input_data_padded, train_gt_data_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulaize trajectory on map\n",
    "path_data = read_data(1)\n",
    "path_input_data, path_gt_data = sort_data_for_usage(TIME_SERIES_SIZE, path_data[0], path_data[1],model.input_shape[2]-18, 1)\n",
    "path_input_data_normalized, path_gt_data_normalized, scaler, gt_scaler = normalize_data(path_input_data, path_gt_data)\n",
    "\n",
    "print(model.summary())\n",
    "print(path_input_data_normalized.shape)\n",
    "\n",
    "prediction = model.predict(path_input_data_normalized)\n",
    "prediction = np.array([gt_scaler.inverse_transform(sample) for sample in prediction])\n",
    "gt = np.array([gt_scaler.inverse_transform(sample) for sample in path_gt_data_normalized])\n",
    "\n",
    "\n",
    "predicted_path = prediction[:,-1,0:3]\n",
    "gt_path = path_gt_data[:,-1,0:3]\n",
    "print(predicted_path.shape)\n",
    "print(gt_path.shape)\n",
    "#plot it on a map\n",
    "\n",
    "\n",
    "m = folium.Map(location=[predicted_path[0,0], predicted_path[0,1]], zoom_start=25, max_zoom=35)\n",
    "\n",
    "for i in range(len(predicted_path)-1):\n",
    "    folium.PolyLine([[predicted_path[i,0], predicted_path[i,1]], [predicted_path[i+1,0], predicted_path[i+1,1]]], color=\"blue\").add_to(m)\n",
    "    folium.PolyLine([[gt_path[i,0], gt_path[i,1]], [gt_path[i+1,0], gt_path[i+1,1]]], color=\"red\").add_to(m)\n",
    "\n",
    "folium.Marker([predicted_path[-1,0], predicted_path[-1,1]], popup=f\"predicted: {predicted_path[-1,0]}, {predicted_path[-1,1]}\", icon=folium.Icon(color=\"red\")).add_to(m)\n",
    "folium.Marker([gt_path[-1,0], gt_path[-1,1]], popup=f\"gt: {gt_path[-1,0]}, {gt_path[-1,1]}\", icon=folium.Icon(color=\"red\")).add_to(m)\n",
    "\n",
    "m.save(\"map.html\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test für Idioten\n",
    "c = np.arange(24).reshape(2,3,4)\n",
    "print(c.shape)\n",
    "for r in c :\n",
    "    print(r)\n",
    "\n",
    "c = c.swapaxes(1,2).swapaxes(2,0)\n",
    "for r in c:\n",
    "    print(r)\n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
